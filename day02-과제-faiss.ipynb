{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import dotenv\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from langchain import hub\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.chains import LLMChain, SequentialChain\n",
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1941, which is longer than the specified 1000\n",
      "Created a chunk of size 1120, which is longer than the specified 1000\n",
      "Created a chunk of size 1912, which is longer than the specified 1000\n",
      "Created a chunk of size 1523, which is longer than the specified 1000\n",
      "Created a chunk of size 1129, which is longer than the specified 1000\n",
      "Created a chunk of size 2242, which is longer than the specified 1000\n",
      "Created a chunk of size 1785, which is longer than the specified 1000\n",
      "Created a chunk of size 1053, which is longer than the specified 1000\n",
      "Created a chunk of size 1454, which is longer than the specified 1000\n",
      "Created a chunk of size 1656, which is longer than the specified 1000\n",
      "Created a chunk of size 2243, which is longer than the specified 1000\n",
      "Created a chunk of size 1086, which is longer than the specified 1000\n",
      "Created a chunk of size 2412, which is longer than the specified 1000\n",
      "Created a chunk of size 1730, which is longer than the specified 1000\n",
      "Created a chunk of size 1123, which is longer than the specified 1000\n",
      "Created a chunk of size 1105, which is longer than the specified 1000\n",
      "Created a chunk of size 1082, which is longer than the specified 1000\n",
      "Created a chunk of size 1570, which is longer than the specified 1000\n",
      "Created a chunk of size 1082, which is longer than the specified 1000\n",
      "Created a chunk of size 1070, which is longer than the specified 1000\n",
      "Created a chunk of size 1529, which is longer than the specified 1000\n",
      "Created a chunk of size 1748, which is longer than the specified 1000\n",
      "Created a chunk of size 1599, which is longer than the specified 1000\n",
      "Created a chunk of size 1351, which is longer than the specified 1000\n",
      "Created a chunk of size 1585, which is longer than the specified 1000\n",
      "Created a chunk of size 1292, which is longer than the specified 1000\n",
      "Created a chunk of size 1388, which is longer than the specified 1000\n",
      "Created a chunk of size 1064, which is longer than the specified 1000\n",
      "Created a chunk of size 1547, which is longer than the specified 1000\n",
      "Created a chunk of size 3395, which is longer than the specified 1000\n",
      "Created a chunk of size 1545, which is longer than the specified 1000\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"), model=\"gpt-4o-mini\")\n",
    "embedding = OpenAIEmbeddings(api_key=os.getenv(\"OPENAI_API_KEY\"), model=\"text-embedding-3-small\")\n",
    "loader = WebBaseLoader([\n",
    "    \"https://applied-llms.org\",\n",
    "])\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = FAISS.from_documents(docs, embedding)\n",
    "retriever = store.as_retriever()\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "input_chain = {\"context\": retriever | format_docs, \"input\": RunnablePassthrough()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG 에 대해 질문 : {'input': 'Explain me what is RAG', 'relevance': 'yes', 'context': '1.2 Information Retrieval / RAG\\nBeyond prompting, another effective way to steer an LLM is by providing knowledge as part of the prompt. This grounds the LLM on the provided context which is then used for in-context learning. This is known as retrieval-augmented generation (RAG). Practitioners have found RAG effective at providing knowledge and improving output, while requiring far less effort and cost compared to finetuning.'}\n",
      "엘든링에 대해 질문 : {'input': 'Explain me what is Elden Ring', 'relevance': 'no', 'context': ''}\n"
     ]
    }
   ],
   "source": [
    "# 라우팅을 위한 관계 정보를 내보내는 절차\n",
    "class RoutingResult(BaseModel):\n",
    "    input: str = Field(description=\"Original user 'input' field\")\n",
    "    relevance: str = Field(description=\"The relevance of the answer to the context it must be 'yes' or 'no\")\n",
    "    context: str = Field(description=\"Original user 'context' field \")\n",
    "\n",
    "routing_parser = JsonOutputParser(pydantic_object=RoutingResult)\n",
    "routing_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "If user input is related to context, answer relevance 'yes', but if not, answer relevance 'no'.\n",
    "If relevance is 'no', context should be empty.\n",
    "context: {context}\n",
    "\n",
    "input: {input}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\",\n",
    "    input_variables=[\"input\"],\n",
    "    partial_variables={\"format_instructions\": routing_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "router_chain = routing_prompt | llm | routing_parser\n",
    "\n",
    "test_chain = input_chain | router_chain\n",
    "\n",
    "print(f'RAG 에 대해 질문 : {test_chain.invoke(\"Explain me what is RAG\")}')\n",
    "print(f'엘든링에 대해 질문 : {test_chain.invoke(\"Explain me what is Elden Ring\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG 에 대해 질문 : {'input': 'Explain me what is RAG', 'relevance': 'yes', 'answer': 'RAG, or retrieval-augmented generation, is a method that enhances the capabilities of language models by providing them with knowledge as part of their prompts. This technique grounds the language model on the given context, allowing it to perform in-context learning. RAG has been found to be effective in delivering knowledge and improving output while being less resource-intensive compared to traditional finetuning methods.', 'context': '1.2 Information Retrieval / RAG\\nBeyond prompting, another effective way to steer an LLM is by providing knowledge as part of the prompt. This grounds the LLM on the provided context which is then used for in-context learning. This is known as retrieval-augmented generation (RAG). Practitioners have found RAG effective at providing knowledge and improving output, while requiring far less effort and cost compared to finetuning.'}\n",
      "엘든링에 대해 질문 : {'input': 'Explain me what is Elden Ring', 'relevance': 'no', 'answer': '', 'context': ''}\n"
     ]
    }
   ],
   "source": [
    "# 응답을 추가해주는 절차\n",
    "class GeneratingResult(BaseModel):\n",
    "    input: str = Field(description=\"Original user 'input' field\")\n",
    "    relevance: str = Field(description=\"Original user 'relevance' field\")\n",
    "    answer: str = Field(description=\"Using 'input' and 'relevance' fields, generate an answer, only if relevance is 'yes'\")\n",
    "    context: str = Field(description=\"Original user 'context' field \")\n",
    "\n",
    "generating_parser = JsonOutputParser(pydantic_object=GeneratingResult)\n",
    "generating_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "Answer the user question only if relevance is 'yes'.\n",
    "When you answer must using context.\n",
    "If relevance is 'no', you can leave the answer field empty.\n",
    "\n",
    "relevance: {relevance}\n",
    "\n",
    "context: {context}\n",
    "\n",
    "input: {input}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\",\n",
    "    input_variables=[\"input\"],\n",
    "    partial_variables={\"format_instructions\": generating_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "generating_chain = generating_prompt | llm | generating_parser\n",
    "\n",
    "test_chain = input_chain | router_chain | generating_chain\n",
    "\n",
    "print(f'RAG 에 대해 질문 : {test_chain.invoke(\"Explain me what is RAG\")}')\n",
    "print(f'엘든링에 대해 질문 : {test_chain.invoke(\"Explain me what is Elden Ring\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG 에 대해 질문 : {'input': 'Explain me what is RAG', 'relevance': 'yes', 'hallucination': 'no', 'answer': 'RAG, or retrieval-augmented generation, is a method that enhances the capabilities of a language model (LLM) by providing it with knowledge as part of the prompt. This approach grounds the LLM in the provided context, allowing it to engage in in-context learning. RAG has been found effective in supplying knowledge and enhancing output while requiring significantly less effort and cost compared to traditional finetuning.', 'context': '1.2 Information Retrieval / RAG\\nBeyond prompting, another effective way to steer an LLM is by providing knowledge as part of the prompt. This grounds the LLM on the provided context which is then used for in-context learning. This is known as retrieval-augmented generation (RAG). Practitioners have found RAG effective at providing knowledge and improving output, while requiring far less effort and cost compared to finetuning.'}\n",
      "엘든링에 대해 질문 : {'input': 'Explain me what is Elden Ring', 'relevance': 'no', 'hallucination': '', 'answer': '', 'context': ''}\n"
     ]
    }
   ],
   "source": [
    "# 응답을 추가해주는 절차\n",
    "class HallucinationResult(BaseModel):\n",
    "    input: str = Field(description=\"Original user 'input' field\")\n",
    "    relevance: str = Field(description=\"Original user 'relevance' field\")\n",
    "    hallucination: str = Field(description=\"The hallucination of the answer it must be 'yes' or 'no'\")\n",
    "    answer: str = Field(description=\"Using 'input' and 'relevance' fields, generate an answer, only if relevance is 'yes' if relevance is 'no' answer must be empty\")\n",
    "    context: str = Field(description=\"Original user 'context' field \")\n",
    "\n",
    "hallucination_parser = JsonOutputParser(pydantic_object=HallucinationResult)\n",
    "hallucination_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "Using context and input, check the answer if the answer is real or hallucination.\n",
    "If answer looks like a hallucination, hallucination must be 'yes', but if not, hallucination must be 'no'.\n",
    "If relevance is 'no', hallucination must be empty.\n",
    "\n",
    "answer: {answer}\n",
    "\n",
    "relevance: {relevance}\n",
    "\n",
    "context: {context}\n",
    "\n",
    "input: {input}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\",\n",
    "    input_variables=[\"input\"],\n",
    "    partial_variables={\"format_instructions\": hallucination_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "hallucination_chain = hallucination_prompt | llm | hallucination_parser\n",
    "\n",
    "test_chain = input_chain | router_chain | generating_chain | hallucination_chain\n",
    "\n",
    "print(f'RAG 에 대해 질문 : {test_chain.invoke(\"Explain me what is RAG\")}')\n",
    "print(f'엘든링에 대해 질문 : {test_chain.invoke(\"Explain me what is Elden Ring\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG 에 대해 질문 : {'input': 'Explain me what is RAG', 'relevance': 'yes', 'hallucination': 'no', 'answer': 'RAG, or retrieval-augmented generation, is a technique that enhances the capabilities of a language model by providing it with relevant knowledge as part of the prompt. This method grounds the model on the supplied context, facilitating in-context learning and leading to more informed and accurate outputs. It is recognized for being effective at providing knowledge and improving output while requiring significantly less effort and cost compared to traditional finetuning methods.', 'context': '1.2 Information Retrieval / RAG Beyond prompting, another effective way to steer an LLM is by providing knowledge as part of the prompt. This grounds the LLM on the provided context which is then used for in-context learning. This is known as retrieval-augmented generation (RAG). Practitioners have found RAG effective at providing knowledge and improving output, while requiring far less effort and cost compared to finetuning.'}\n",
      "엘든링에 대해 질문 : {'input': 'Explain me what is Elden Ring', 'relevance': 'no', 'hallucination': '', 'answer': '', 'context': ''}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class RecursiveHResult(BaseModel):\n",
    "    input: str = Field(description=\"Original user 'input' field\")\n",
    "    relevance: str = Field(description=\"Original user 'relevance' field\")\n",
    "    hallucination: str = Field(description=\"The hallucination of the answer it must be 'yes' or 'no'\")\n",
    "    answer: str = Field(description=\"Regenerate Only if hallucination is 'yes'\")\n",
    "    context: str = Field(description=\"Original user 'context' field \")\n",
    "\n",
    "recursive_h_parser = JsonOutputParser(pydantic_object=RecursiveHResult)\n",
    "recursive_h_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "Using context and input, check the answer if the answer is real or hallucination.\n",
    "\n",
    "If relevance is 'no', hallucination must be empty. and aswer must be empty.\n",
    "\n",
    "If relevance is 'yes', answer must be generated. \n",
    "But this time, change answer only when hallucination is 'yes'.\n",
    "If relevance is 'yes' and hallucination is 'no', answer must be the same as the previous answer.\n",
    "Else if relevance is 'yes' and hallucination is 'yes', answer must be regenerated.\n",
    "\n",
    "\n",
    "answer: {answer}\n",
    "\n",
    "relevance: {relevance}\n",
    "\n",
    "hallucination: {hallucination}\n",
    "\n",
    "context: {context}\n",
    "\n",
    "input: {input}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\",\n",
    "    input_variables=[\"input\"],\n",
    "    partial_variables={\"format_instructions\": recursive_h_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "recursive_h_chain = recursive_h_prompt | llm | recursive_h_parser\n",
    "\n",
    "test_chain = input_chain | router_chain | generating_chain | hallucination_chain | recursive_h_chain | recursive_h_chain\n",
    "\n",
    "print(f'RAG 에 대해 질문 : {test_chain.invoke(\"Explain me what is RAG\")}')\n",
    "print(f'엘든링에 대해 질문 : {test_chain.invoke(\"Explain me what is Elden Ring\")}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
